<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MediaPipe Hands & Face Mesh</title>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js" crossorigin="anonymous"></script>
  <!-- Importa o arquivo gestos.js -->
  <script type="module" src="gestos.js"></script>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="container">
    <video class="input_video" autoplay></video>
    <canvas class="output_canvas" width="1280" height="720"></canvas>
  </div>
  <div id="translation">
    Aguardando gesto...
  </div>


  <script type="module">
    import { gestoAmor,
             gestoOi,
             gestoA,
             gestoB,
             gestoC,
             gestoD,
             gestoE,
             detectGestoStable } from './gestos.js';
  
    const videoElement = document.querySelector('.input_video');
    const canvasElement = document.querySelector('.output_canvas');
    const canvasCtx = canvasElement.getContext('2d');
    const translationElement = document.getElementById('translation');
    let lastLandmarks = null;
  
    // === Função para desenhar landmarks das mãos ===
    function onHandsResults(results) {
      canvasCtx.save();
      canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
      canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);
  
      if (results.multiHandLandmarks) {
        for (const landmarks of results.multiHandLandmarks) {
          drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, { color: '#00FF00', lineWidth: 5 });
          drawLandmarks(canvasCtx, landmarks, { color: '#FF0000', lineWidth: 2 });
  
          const currentTime = Date.now();
          const detectedGesto = detectGestoStable(landmarks, currentTime);
  
          if (detectedGesto) {
            translationElement.textContent = detectedGesto;
            console.log(`Gesto detectado: ${detectedGesto}`);
          }
        }
      }
      canvasCtx.restore();
    }
  
    // === Função para lidar com Face Mesh (sem desenhar landmarks) ===
    function onFaceMeshResults(results) {
      // Apenas processa os resultados, sem desenhar nada
    }
  
    // === Configuração do Hands ===
    const hands = new Hands({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`,
    });
    hands.setOptions({
      maxNumHands: 2,
      modelComplexity: 1,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5,
    });
    hands.onResults(onHandsResults);
  
    // === Configuração do Face Mesh ===
    const faceMesh = new FaceMesh({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
    });
    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5,
    });
    faceMesh.onResults(onFaceMeshResults);
  
    // === Configuração da câmera para processar ambos ===
    const camera = new Camera(videoElement, {
      onFrame: async () => {
        // Envia os frames para ambas as detecções
        await hands.send({ image: videoElement });
        await faceMesh.send({ image: videoElement });
      },
      width: 1280,
      height: 720,
    });
    camera.start();
  </script>
  
</body>
</html>